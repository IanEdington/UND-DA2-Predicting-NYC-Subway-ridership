{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the NYC Subway Dataset\n",
    "## by Ian Edington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Section 0. References\n",
    "####References used for this project:\n",
    "1. https://en.wikipedia.org/wiki/Mann–Whitney_U_test\n",
    "2. https://statistics.laerd.com/spss-tutorials/mann-whitney-u-test-using-spss-statistics.php\n",
    "3. http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n",
    "4. http://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\n",
    "5. http://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html\n",
    "6. http://pandas.pydata.org/pandas-docs/stable/visualization.html#histograms\n",
    "7. http://pandas.pydata.org/pandas-docs/stable/groupby.html\n",
    "8. http://pypi.python.org/pypi/ggplot/\n",
    "9. http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "10. http://stackoverflow.com/questions/12190874/pandas-sampling-a-dataframe\n",
    "11. http://stackoverflow.com/questions/19711943/pandas-dataframe-to-dictionary-value\n",
    "12. http://stackoverflow.com/questions/7001606/json-serialize-a-dictionary-with-tuples-as-key\n",
    "13. http://statsmodels.sourceforge.net/0.5.0/generated/statsmodels.regression.linear_model.OLS.html\n",
    "14. http://statsmodels.sourceforge.net/0.5.0/generated/statsmodels.regression.linear_model.OLS.fit.html\n",
    "15. http://statsmodels.sourceforge.net/0.5.0/generated/statsmodels.regression.linear_model.RegressionResults.html\n",
    "16. http://wiki.scipy.org/Cookbook/Matplotlib/BarCharts\n",
    "17. http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Section 1. Statistical Test\n",
    "####1.1 Which statistical test did you use to analyze the NYC subway data?\n",
    "The Mann-Whitney U test was used to determine if ridership on days where there was rainfall was significantly different than ridership on days where there was no rainfall. This same test was used to determine if ridership on days where there was fog was significantly different than ridership on days where there was no fog.\n",
    "####Did you use a one-tail or a two-tail P value?\n",
    "I used two-tailed P values in order to determine directionality.\n",
    "####What is the null hypothesis?\n",
    "Ridership does not change based on if it’s raining or not that day.\n",
    "\n",
    "H0: μrain = μno rain\n",
    "H0: μfog = μno fog\n",
    "\n",
    "####What is your p-critical value?\n",
    "0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1.2 Why is this statistical test applicable to the dataset? In particular, consider the assumptions that the test is making about the distribution of ridership in the two samples.\n",
    "\n",
    "This test is applicable because the assumptions made by the Mann- Whitney U test are true about this data and because the question we were asking can be answered by this test.\n",
    "Based on Wikipedia’s articleA these are the assumptions made by the\n",
    "Mann-Whitney U test and evidence that this data set conforms to these assumptions.\n",
    "1. All the observations from both groups are independent of each other:\n",
    "We assume that the ridership of one hour is not based on the ridership of the previous hour or the previous day. This is a reasonable assumption since\n",
    "2. The responses are at least ordinal: The dependent variable ENTRIESn_hourly is a continuous range of positive whole numbers.\n",
    "3. The null hypothesis H0 is \"The distributions of both populations are equal\"\n",
    "4. The alternative hypothesis H1 is \"the probability of an observation from the population X exceeding an observation from the second population Y is different from the probability of an observation from Y exceeding an observation from X : P(X>Y)≠P(Y>X).”\n",
    "From Laerd StatisticsB we have two additional assumptions:\n",
    "5. You have one independent variable that consists of two categorical, independent groups:\n",
    "This is true for both rain and fog variables: 1 and 0\n",
    "6. You must determine whether the distribution of scores for both groups of your independent variable have the same shape or a different shape.\n",
    "We can see based on the histograms for both rain and fog that the distributions have the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1.3 What results did you get from this statistical test? P-values and the means for each of the two samples under test.\n",
    "P-Value\n",
    "Mean of good weather days\n",
    "Mean of rainy of fogy days\n",
    "Rain\n",
    "0.0249999 1090 1105\n",
    "Fog\n",
    "0.0000061 1083 1155\n",
    "\n",
    "####1.4 What is the significance and interpretation of these results?\n",
    "For the case of both Rain and Fog we reject the null hypothesis. This means that when it is raining the NYC subway is likely to have a hirer ridership than when it is not raining. Likewise for fog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import problem_set_answers as a\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "######################\n",
    "### Wrangling data ###\n",
    "######################\n",
    "\n",
    "#-- Read data into pandas from CSV\n",
    "# data = pd.read_csv(r'turnstile_data_master_with_weather.csv')\n",
    "# data = pd.read_csv(r'turnstile_weather_v2.csv')\n",
    "\n",
    "#-- Modity data to add a day of the week, then save to new CSV\n",
    "# data['day_of_week'] = pd.to_datetime(data['DATEn']).dt.dayofweek\n",
    "# data.to_csv(path_or_buf=r'turnstile_data_working_copy.csv')\n",
    "#-- Use the data with day of week already there\n",
    "data = pd.read_csv(r'turnstile_data_working_copy.csv')\n",
    "del data['Unnamed: 0']\n",
    "\n",
    "######################\n",
    "### Exploring data ###\n",
    "######################\n",
    "\n",
    "# ### What Features might be affecting ridership?\n",
    "\n",
    "# all possible Features:\n",
    "# print (data.columns.values.tolist())\n",
    "#> ['Unnamed: 0', 'Unnamed: 0.1', 'UNIT', 'DATEn', 'TIMEn', 'Hour', 'DESCn', 'ENTRIESn_hourly', 'EXITSn_hourly', 'maxpressurei', 'maxdewpti', 'mindewpti', 'minpressurei', 'meandewpti', 'meanpressurei', 'fog', 'rain', 'meanwindspdi', 'mintempi', 'meantempi', 'maxtempi', 'precipi', 'thunder', 'day_of_week']\n",
    "\n",
    "# Which ones should we explore?\n",
    "# UNIT: YES. Using UNIT would tell us if their is a specific area that is being used more than another not weather related we want to predict ridership overall.\n",
    "# DATEn: NO. Would tell us if specific days are used more than others. This would be useful if we could seperate days of the week.(Done) Use day_of_week instead.\n",
    "# day_of_week: YES. this is DATEn in usable format\n",
    "# TIMEn: NO. Would be useful if it was grouped by hours. Use Hour instead.\n",
    "# Hour: YES. this is TIMEn in usable format\n",
    "# YES to all weather data except thunder:\n",
    "#     'maxpressurei', 'maxdewpti', 'mindewpti', 'minpressurei', 'meandewpti', 'meanpressurei', 'fog', 'rain', 'meanwindspdi', 'mintempi', 'meantempi', 'maxtempi', 'precipi'\n",
    "\n",
    "# DESCn: NO. It is only ever REGULAR\n",
    "# ENTRIESn_hourly: NO. primary dependent variable\n",
    "# EXITSn_hourly: NO. secondary dependent variable\n",
    "# thunder: NO. Only ever 0\n",
    "\n",
    "# features_to_explor = ['UNIT', 'day_of_week', 'Hour', 'maxpressurei', 'maxdewpti', 'mindewpti', 'minpressurei', 'meandewpti', 'meanpressurei', 'fog', 'rain', 'meanwindspdi', 'mintempi', 'meantempi', 'maxtempi', 'precipi']\n",
    "\n",
    "\n",
    "### Plot the interesting factors against ENTRIESn_hourly\n",
    "# Figures generated are in dir EDA_figs\n",
    "\n",
    "# for feature in features_to_explore:\n",
    "# \ta.bar_plot_mean_Entries(data, feature)\n",
    "\n",
    "\n",
    "### Looking at only the binary factors (Rain and Fog)\n",
    "#-- Select out the two sections for each one:\n",
    "no_rain = data[data.rain == 0]['ENTRIESn_hourly']\n",
    "rain = data[data.rain == 1]['ENTRIESn_hourly']\n",
    "no_fog = data[data.fog == 0]['ENTRIESn_hourly']\n",
    "fog = data[data.fog == 1]['ENTRIESn_hourly']\n",
    "\n",
    "#-- Test selection: make a hist of the data\n",
    "a.hist_MWW_suitability(no_rain, rain, rORf='rain')\n",
    "a.hist_MWW_suitability(no_fog, fog, rORf='fog')\n",
    "\n",
    "#-- compare binary factors using Mann-Whitney statistic\n",
    "# bi_f = {'title': ['mean with_rain',\n",
    "# \t\t\t\t  'mean without_rain',\n",
    "# \t\t\t\t  'Mann-Whitney U-statistic',\n",
    "# \t\t\t\t  'Mann-Whitney p-value' ]}\n",
    "# bi_f['rain'] = list(a.mann_whitney_plus_means(no_rain, rain))\n",
    "# bi_f['fog'] = list(a.mann_whitney_plus_means(no_fog, fog))\n",
    "# pprint.pprint (bi_f)\n",
    "\t# >{'title': ['mean with_rain',\n",
    "\t#            'mean without_rain',\n",
    "\t#            'Mann-Whitney U-statistic',\n",
    "\t#            'Mann-Whitney p-value'],\n",
    "\t#  'fog': [1083.4492820876781,\n",
    "\t#          1154.6593496303688,\n",
    "\t#          1189034717.5,\n",
    "\t#          6.0915569104373036e-06],\n",
    "\t#  'rain': [1090.278780151855,\n",
    "\t#           1105.4463767458733,\n",
    "\t#           1924409167.0,\n",
    "\t#           0.024999912793489721]}\n",
    "\n",
    "###\n",
    "\n",
    "#########################\n",
    "### Linear Regression ###\n",
    "#########################\n",
    "\n",
    "### Prepare data for linear regression ###\n",
    "#-- Create dummy varriables for 'UNIT'\n",
    "# data, UNIT_dummy = a.UNIT_dummy_vars(data)\n",
    "\n",
    "## save the dummy variables used for reference\n",
    "# with open('UNIT_dummy.json', 'wr') as f:\n",
    "#     json.dump(UNIT_dummy, f)\n",
    "\n",
    "## save over working file\n",
    "# data['day_of_week'] = pd.to_datetime(data['DATEn']).dt.dayofweek\n",
    "# data.to_csv(path_or_buf=r'turnstile_data_working_copy.csv')\n",
    "\n",
    "## Add UNIT_dummy and day_of_week columns for test_data\n",
    "# test_data = pd.read_csv(r'turnstile_weather_v2.csv')\n",
    "# with open('UNIT_dummy.json') as f:\n",
    "#     UNIT_dummy = json.load(f)\n",
    "# test_data, UNIT_dummy = a.UNIT_dummy_vars(test_data, UNIT_dummy)\n",
    "# test_data['day_of_week'] = pd.to_datetime(test_data['DATEn']).dt.dayofweek\n",
    "# change hour to Hour so data & test_data match\n",
    "# test_data = test_data.rename(columns={'hour': 'Hour'})\n",
    "# test_data = test_data.rename(columns={'meanwspdi': 'meanwindspdi'})\n",
    "# test_data.to_csv(path_or_buf = r'turnstile_weather_v2_working_copy.csv')\n",
    "\n",
    "\n",
    "### Find the right features to use\n",
    "\n",
    "#-- List of features to explore\n",
    "## The following columns were removed due to lack of test data: 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti', 'maxtempi'\n",
    "# features_to_explore = ['UNIT_dummy', 'day_of_week', 'Hour', 'meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'precipi']\n",
    "\n",
    "#-- save reslts in a list in the form (r_squared, [feature list], (intercept, params))\n",
    "# results = [('r_squared', ('feature','list'), ('intercept', 'params')),]\n",
    "\n",
    "## reload the data\n",
    "# data = pd.read_csv(r'turnstile_data_working_copy.csv')\n",
    "# del data['Unnamed: 0']\n",
    "# test_data = pd.read_csv(r'turnstile_weather_v2_working_copy.csv')\n",
    "# del test_data['Unnamed: 0']\n",
    "\n",
    "## Create numpy arrays\n",
    "# values_array = data['ENTRIESn_hourly'].values\n",
    "# test_values_array = test_data['ENTRIESn_hourly'].values\n",
    "\n",
    "### Test every variable independently against the\n",
    "\n",
    "# for feature in features_to_explore:\n",
    "#     #-- extract feature\n",
    "#     #-- generate predictions\n",
    "#     feature_array = data[feature].values\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#\n",
    "#     #-- calculate r** using backup data\n",
    "#     test_feature_array = test_data[feature].values\n",
    "#     predictions = test_feature_array * params + intercept\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, ([feature],), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "    # [('r_squared', ('feature', 'list'), ('intercept', 'params')),\n",
    "    #  (0.36066468329622159, (['UNIT_dummy'],), (0.44582044219855199, (1.000019654833768,))),\n",
    "    #  (-0.062813493262236841, (['day_of_week'],), (1350.5997806795308, (-85.5451482036722,))),\n",
    "    #  (-0.020645166615559596, (['Hour'],), (447.17776398603291, (59.48616831074126,))),\n",
    "    #  (-0.072148096599663258, (['meanpressurei'],), (9752.6656708963455, (-288.91356369944765,))),\n",
    "    #  (-0.073920312745420658, (['fog'],), (1083.449282087679, (71.21006754320405,))),\n",
    "    #  (-0.072021772467172562, (['rain'],), (1090.2787801517247, (15.167596594046824,))),\n",
    "    #  (-0.067979992755955676, (['meanwindspdi'],), (921.35747190610505, (31.388951556096405,))),\n",
    "    #  (-0.069465093618023444, (['meantempi'],), (1616.6334487585414, (-8.11089419107536,))),\n",
    "    #  (-0.073463327604493589, (['precipi'],), (1086.2781786381292, (52.64972158324689,)))]\n",
    "\n",
    "\n",
    "################################\n",
    "### Linear Regression Take 2 ###\n",
    "################################\n",
    "\n",
    "### Well that didn't work :( (lots of -ve R values)\n",
    "### going to try again with more dummy variables, SGD & spliting the data into test data and learning data instead of using the second data set.\n",
    "# http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "### Prepare data for linear regression ###\n",
    "## add more dummy units for 'UNIT', 'day_of_week', 'Hour' using mean ENTRIESn_hourly\n",
    "# use original data\n",
    "# data = pd.read_csv(r'turnstile_data_master_with_weather.csv')\n",
    "# data['day_of_week'] = pd.to_datetime(data['DATEn']).dt.dayofweek\n",
    "# del data['Unnamed: 0']\n",
    "\n",
    "# mean dummy set for UNIT, day_of_week, & Hour\n",
    "# data, UNIT_means = a.mean_dummy_units(data, feature='UNIT')\n",
    "# data, day_of_week_means = a.mean_dummy_units(data, feature='day_of_week')\n",
    "# data, Hour_means = a.mean_dummy_units(data, feature='Hour')\n",
    "\n",
    "## save data to working data\n",
    "# data.to_csv(path_or_buf=r'turnstile_data_working_copy.csv')\n",
    "\n",
    "## save the dummy variables used for reference\n",
    "# mean_dummys = {'UNIT_means': a.JSONify_dict(UNIT_means),\n",
    "#                'day_of_week_means': a.JSONify_dict(day_of_week_means),\n",
    "#                'Hour_means': a.JSONify_dict(Hour_means)}\n",
    "# with open('mean_dummys.json', 'w') as f:\n",
    "#     json.dump(mean_dummys, f)\n",
    "\n",
    "## reload the data without test data\n",
    "# data = pd.read_csv(r'turnstile_data_working_copy.csv')\n",
    "# del data['Unnamed: 0']\n",
    "\n",
    "## split data into training_data and test_data\n",
    "# http://stackoverflow.com/questions/12190874/pandas-sampling-a-dataframe\n",
    "# test_rows = np.random.choice(data.index.values, len(data)*0.1, replace=False)\n",
    "# with open('test_rows.json', 'w') as f:\n",
    "#     json.dump(test_rows.tolist(), f)\n",
    "\n",
    "# Split data from test_rows\n",
    "# with open('test_rows.json') as f:\n",
    "#     test_rows = json.load(f)\n",
    "# test_data = data.ix[test_rows]\n",
    "# training_data = data.drop(test_rows)\n",
    "\n",
    "\n",
    "### Find the right features to use\n",
    "#-- save reslts in a list in the form (r_squared, [feature list], (intercept, params))\n",
    "# results = [('r_squared', ('feature','list'), ('intercept', 'params')),]\n",
    "\n",
    "## Create numpy arrays\n",
    "# values_array = training_data['ENTRIESn_hourly'].values\n",
    "# test_values_array = test_data['ENTRIESn_hourly'].values\n",
    "\n",
    "### Test every variable independently against the\n",
    "#-- List of features to explore\n",
    "# features_to_explore = ['UNIT', 'day_of_week', 'Hour', 'meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'precipi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti', 'maxtempi', 'UNIT_means', 'day_of_week_means', 'Hour_means']\n",
    "# for feature in features_to_explore:\n",
    "#     #-- extract feature\n",
    "#     if feature in ['UNIT', 'day_of_week', 'Hour']:\n",
    "#         #-- Use pandas.get_dummies for UNIT, day_of_week, Hour\n",
    "#         feature_array = pd.get_dummies(training_data[feature], prefix=feature)\n",
    "#         test_feature_array = pd.get_dummies(test_data[feature], prefix=feature)\n",
    "#         dot = np.dot\n",
    "#     else:\n",
    "#         feature_array = training_data[feature].values\n",
    "#         test_feature_array = test_data[feature].values\n",
    "#         def dot(a,b):return a*b\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#     predictions = dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, ([feature],), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0][0]] for x in results])\n",
    "    #>[['r_squared', 'f'],\n",
    "    #  [0.41004488886519175, 'UNIT'],\n",
    "    #  [0.013480119273690194, 'day_of_week'],\n",
    "    #  [0.11918213646536135, 'Hour'],\n",
    "    #  [4.6893252414248465e-05, 'meanpressurei'],\n",
    "    #  [0.00010419321888721633, 'fog'],\n",
    "    #  [-0.00016555909505555633, 'rain'],\n",
    "    #  [9.5051826772496462e-05, 'meanwindspdi'],\n",
    "    #  [0.0002848165517614909, 'meantempi'],\n",
    "    #  [-0.00018163984433350322, 'precipi'],\n",
    "    #  [9.0488266448751631e-06, 'maxpressurei'],\n",
    "    #  [-0.0001768707542453285, 'maxdewpti'],\n",
    "    #  [0.00058739536864838016, 'mintempi'],\n",
    "    #  [-0.00018067147690326024, 'mindewpti'],\n",
    "    #  [0.00028448751528775684, 'minpressurei'],\n",
    "    #  [-0.00019278784489529244, 'meandewpti'],\n",
    "    #  [-6.3230968838645651e-07, 'maxtempi'],\n",
    "    #  [0.41547200266629658, 'UNIT_means'],\n",
    "    #  [0.013506567080348253, 'day_of_week_means'],\n",
    "    #  [0.11935876457150507, 'Hour_means']]\n",
    "\n",
    "### Using dummy variables & OLS still does not result in good predictions\n",
    "### Implementing SGD...\n",
    "\n",
    "# for feature in features_to_explore:\n",
    "#     #-- extract feature\n",
    "#     if feature in ['UNIT', 'day_of_week', 'Hour']:\n",
    "#         #-- Use pandas.get_dummies for UNIT, day_of_week, Hour\n",
    "#         feature_array = pd.get_dummies(training_data[feature], prefix=feature)\n",
    "#         test_feature_array = pd.get_dummies(test_data[feature], prefix=feature)\n",
    "#         dot = np.dot\n",
    "#     else:\n",
    "#         feature_array = training_data[feature].values\n",
    "#         test_feature_array = test_data[feature].values\n",
    "#         def dot(a,b):return a*b\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.SGD_regression(feature_array, values_array)\n",
    "#     predictions = dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, ([feature],), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0][0]] for x in results]) # compare to 0.41004488886519175\n",
    "    #>[['r_squared', 'f'],\n",
    "    #  [0.35693821347008892, 'UNIT'],\n",
    "    #  [-0.13336887490640703, 'day_of_week'],\n",
    "    #  [0.054474268865662423, 'Hour']]\n",
    "\n",
    "### Test the best variable with one other variable at a time\n",
    "## Eliminate 'UNIT', 'day_of_week', 'Hour' from the possible features because the mean dummy units predict better.\n",
    "## Remove UNIT_means and add it to all feature sets\n",
    "# features_to_explore = ['meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'precipi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti', 'maxtempi', 'day_of_week_means', 'Hour_means']\n",
    "#\n",
    "# for feature in features_to_explore:\n",
    "#     features = [feature,'UNIT_means']\n",
    "#     #-- extract feature\n",
    "#     feature_array = training_data[features].values\n",
    "#     test_feature_array = test_data[features].values\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#     predictions = np.dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, (features,), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0]] for x in results]) # compare to 0.41004488886519175\n",
    "    #>[['r_squared', 'feature'],\n",
    "    #  [0.41559594882953754, ['meanpressurei', 'UNIT_means']],\n",
    "    #  [0.41558729341793865, ['fog', 'UNIT_means']],\n",
    "    #  [0.4154506737493936, ['rain', 'UNIT_means']],\n",
    "    #  [0.41601434244915769, ['meanwindspdi', 'UNIT_means']],\n",
    "    #  [0.41603923560882017, ['meantempi', 'UNIT_means']],\n",
    "    #  [0.41529264612541006, ['precipi', 'UNIT_means']],\n",
    "    #  [0.41554291112944919, ['maxpressurei', 'UNIT_means']],\n",
    "    #  [0.41563415696138684, ['maxdewpti', 'UNIT_means']],\n",
    "    #  [0.41638196279951056, ['mintempi', 'UNIT_means']],\n",
    "    #  [0.41587675542449065, ['mindewpti', 'UNIT_means']],\n",
    "    #  [0.4158260985182054, ['minpressurei', 'UNIT_means']],\n",
    "    #  [0.41576041540946318, ['meandewpti', 'UNIT_means']],\n",
    "    #  [0.41571645197371865, ['maxtempi', 'UNIT_means']],\n",
    "    #  [0.42824082804989616, ['day_of_week_means', 'UNIT_means']],\n",
    "    #  [0.47968730654524339, ['Hour_means', 'UNIT_means']]]\n",
    "\n",
    "### Test Hour_means & UNIT_means with one other variable at a time\n",
    "## Remove UNIT_means & Hour_means and add them to all feature sets\n",
    "# features_to_explore = ['meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'precipi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti', 'maxtempi', 'day_of_week_means']\n",
    "#\n",
    "# for feature in features_to_explore:\n",
    "#     features = [feature, 'UNIT_means', 'Hour_means']\n",
    "#     #-- extract feature\n",
    "#     feature_array = training_data[features].values\n",
    "#     test_feature_array = test_data[features].values\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#     predictions = np.dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, (features,), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0]] for x in results]) # compare to 0.47968730654524339\n",
    "    #>[['r_squared', 'feature'],\n",
    "    #  [0.47973690168712868, ['meanpressurei', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47979460100406623, ['fog', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47967663503601432, ['rain', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.48036013058982785, ['meanwindspdi', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.4800944047778759, ['meantempi', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47955870644631726, ['precipi', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47967275545351939, ['maxpressurei', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47977675413800425, ['maxdewpti', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.48037894348391585, ['mintempi', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47996890142867377, ['mindewpti', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47996153050185486, ['minpressurei', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47986210732408541, ['meandewpti', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.47984846041148965, ['maxtempi', 'UNIT_means', 'Hour_means']],\n",
    "    #  [0.49370326835188771, ['day_of_week_means', 'UNIT_means', 'Hour_means']]]\n",
    "\n",
    "### Test UNIT_means, Hour_means & day_of_week_means with one other variable at a time\n",
    "## Remove UNIT_means, Hour_means & day_of_week_means and add them to all feature sets\n",
    "# features_to_explore = ['meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'precipi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti', 'maxtempi']\n",
    "#\n",
    "# for feature in features_to_explore:\n",
    "#     features = [feature, 'UNIT_means', 'Hour_means', 'day_of_week_means']\n",
    "#     #-- extract feature\n",
    "#     feature_array = training_data[features].values\n",
    "#     test_feature_array = test_data[features].values\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#     predictions = np.dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, (features,), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0]] for x in results])\n",
    "    #>[['r_squared', 'feature'],\n",
    "    #  [0.49370744074028183, ['meanpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49372753865892172, ['fog', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49378270281459169, ['rain', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49372031991677812, ['meanwindspdi', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49394608711807597, ['meantempi', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.4938495272449126,  ['precipi', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49367905881773144, ['maxpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49376195072109108, ['maxdewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49389470802005497, ['mintempi', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49373900639359747, ['mindewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49378156919660487, ['minpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49373445063798294, ['meandewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means']],\n",
    "    #  [0.49397803935218865, ['maxtempi', 'UNIT_means', 'Hour_means', 'day_of_week_means']]]\n",
    "\n",
    "## Test adding maxtempi\n",
    "# features_to_explore = ['meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'precipi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti']\n",
    "#\n",
    "# for feature in features_to_explore:\n",
    "#     features = [feature, 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']\n",
    "#     #-- extract feature\n",
    "#     feature_array = training_data[features].values\n",
    "#     test_feature_array = test_data[features].values\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#     predictions = np.dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, (features,), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0]] for x in results])\n",
    "    #>[['r_squared', 'feature'],\n",
    "    #  [0.49397619378619528, ['meanpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49399801652926134, ['fog', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49415940953240234, ['rain', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49397862264605163, ['meanwindspdi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49395867822271988, ['meantempi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49430849100089047, ['precipi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49394595436532107, ['maxpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49397478619625901, ['maxdewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49397076793747408, ['mintempi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49394704683251178, ['mindewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49404619381526171, ['minpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']],\n",
    "    #  [0.49395595938817671, ['meandewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi']]]\n",
    "\n",
    "# Test adding precipi\n",
    "# features_to_explore = ['meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti']\n",
    "#\n",
    "# for feature in features_to_explore:\n",
    "#     features = [feature, 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']\n",
    "#     #-- extract feature\n",
    "#     feature_array = training_data[features].values\n",
    "#     test_feature_array = test_data[features].values\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#     predictions = np.dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, (features,), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0]] for x in results])\n",
    "    #>[['r_squared', 'feature'],\n",
    "    #  [0.49435730653077181, ['meanpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.49460171140718767, ['fog', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.4942978801376432, ['rain', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.49431046392161515, ['meanwindspdi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.49427944872247853, ['meantempi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.4942937903172262, ['maxpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.49433418773417048, ['maxdewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.49427944658657152, ['mintempi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.49424410869396584, ['mindewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.49442291041055964, ['minpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']],\n",
    "    #  [0.49427915776113918, ['meandewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi']]]\n",
    "\n",
    "# Test adding fog compare with 0.49460171140718767\n",
    "# features_to_explore = ['meanpressurei', 'rain', 'meanwindspdi', 'meantempi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti']\n",
    "#\n",
    "# for feature in features_to_explore:\n",
    "#     features = [feature, 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']\n",
    "#     #-- extract feature\n",
    "#     feature_array = training_data[features].values\n",
    "#     test_feature_array = test_data[features].values\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#     predictions = np.dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, (features,), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0]] for x in results])\n",
    "    #>[['r_squared', 'feature'],\n",
    "    #  [0.49459748045790541, ['meanpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.49465295145307331, ['rain', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.49462377994181617, ['meanwindspdi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.49456993308680874, ['meantempi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.49457760432961051, ['maxpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.49458594651000443, ['maxdewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.49458519727104444, ['mintempi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.494524766286221, ['mindewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.49461007182343686, ['minpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']],\n",
    "    #  [0.49454481157040175, ['meandewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog']]]\n",
    "\n",
    "# Test adding rain compare with 0.49465295145307331\n",
    "# features_to_explore = ['meanpressurei', 'meanwindspdi', 'meantempi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti']\n",
    "#\n",
    "# for feature in features_to_explore:\n",
    "#     features = [feature, 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']\n",
    "#     #-- extract feature\n",
    "#     feature_array = training_data[features].values\n",
    "#     test_feature_array = test_data[features].values\n",
    "#\n",
    "#     #-- generate predictions\n",
    "#     intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "#     predictions = np.dot(test_feature_array, params) + intercept\n",
    "#     #-- calculate r** using backup data\n",
    "#     r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#     #-- append results to list\n",
    "#     results.append((r_squared, (features,), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0]] for x in results])\n",
    "    #>[['r_squared', 'feature'],\n",
    "    #  [0.4946525609002429, ['meanpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']],\n",
    "    #  [0.49466806849051093, ['meanwindspdi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']],\n",
    "    #  [0.49462569628221476, ['meantempi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']],\n",
    "    #  [0.49462586265978714, ['maxpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']],\n",
    "    #  [0.49469115125765351, ['maxdewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']],\n",
    "    #  [0.4946351051575969, ['mintempi', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']],\n",
    "    #  [0.49461647391320085, ['mindewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']],\n",
    "    #  [0.49466634958996603, ['minpressurei', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']],\n",
    "    #  [0.49464460779303077, ['meandewpti', 'UNIT_means', 'Hour_means', 'day_of_week_means', 'maxtempi', 'precipi', 'fog', 'rain']]]\n",
    "\n",
    "### Try it with all features to see if it makes a difference.\n",
    "# features = ['UNIT_means', 'Hour_means', 'day_of_week_means', 'meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'precipi', 'maxpressurei', 'maxdewpti', 'mintempi',  'mindewpti', 'minpressurei', 'meandewpti', 'maxtempi']\n",
    "#\n",
    "# #-- extract feature\n",
    "# feature_array = training_data[features].values\n",
    "# test_feature_array = test_data[features].values\n",
    "#\n",
    "# #-- generate predictions\n",
    "# intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "# predictions = np.dot(test_feature_array, params) + intercept\n",
    "# #-- calculate r** using backup data\n",
    "# r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "# #-- append results to list\n",
    "# results.append((r_squared, (features,), (intercept, tuple(params.tolist()))))\n",
    "\n",
    "# print ([[x[0], x[1][0]] for x in results])\n",
    "    #>[['r_squared', 'feature'],\n",
    "    #  [0.4953608732252669, ['UNIT_means', 'Hour_means', 'day_of_week_means', 'meanpressurei', 'fog', 'rain', 'meanwindspdi', 'meantempi', 'precipi', 'maxpressurei', 'maxdewpti', 'mintempi', 'mindewpti', 'minpressurei', 'meandewpti', 'maxtempi']]]\n",
    "\n",
    "# Save results to json\n",
    "# with open('results.json', 'w') as f:\n",
    "#     json.dump(results, f)\n",
    "#\n",
    "#\n",
    "# features = ['UNIT_means', 'Hour_means', 'day_of_week_means']\n",
    "# #-- extract feature\n",
    "# feature_array = training_data[features].values\n",
    "# test_feature_array = test_data[features].values\n",
    "#\n",
    "# intercept, params = a.OLS_linear_regression(feature_array, values_array)\n",
    "# predictions = np.dot(test_feature_array, params) + intercept\n",
    "# #-- calculate r** using backup data\n",
    "# r_squared = a.compute_r_squared(test_values_array, predictions)\n",
    "#\n",
    "# plt = a.plot_residuals(test_data, predictions)\n",
    "# plt.show()\n",
    "\n",
    "data_to_plot = [data[data.day_of_week==0]['ENTRIESn_hourly'],\n",
    "                data[data.day_of_week==1]['ENTRIESn_hourly'],\n",
    "                data[data.day_of_week==2]['ENTRIESn_hourly'],\n",
    "                data[data.day_of_week==3]['ENTRIESn_hourly'],\n",
    "                data[data.day_of_week==4]['ENTRIESn_hourly'],\n",
    "                data[data.day_of_week==5]['ENTRIESn_hourly'],\n",
    "                data[data.day_of_week==6]['ENTRIESn_hourly'],]\n",
    "a.boxplot_date_time(data_to_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
